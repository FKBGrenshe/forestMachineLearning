{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BETH Anomaly Detection — End‑to‑End Notebook\n",
        "*Generated on:* 2025-10-31 08:02:30\n",
        "\n",
        "本 Notebook 覆盖完整流程：数据读取 → 预处理 → 训练 → 验证 → 测试 → 生成 submission CSV。\n",
        "默认模型：`IsolationForest`（无监督）。你可以在配置区修改路径与参数。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. 环境检查"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: 3.10.16\n",
            "OS: Windows-10-10.0.19041-SP0\n",
            "NumPy: 2.1.2\n",
            "pandas: 2.2.3\n",
            "scikit-learn: 1.7.2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import sys, os, platform, numpy as np, pandas as pd\n",
        "import sklearn\n",
        "print('Python:', sys.version.split()[0])\n",
        "print('OS:', platform.platform())\n",
        "print('NumPy:', np.__version__)\n",
        "print('pandas:', pd.__version__)\n",
        "print('scikit-learn:', sklearn.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 配置区（路径与超参数）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === 路径设置（按需修改）===\n",
        "DATA_DIR = \"/mnt/data\"  # 你的数据目录\n",
        "TRAIN_CSV = r\"D:\\Project\\MachineLearningAlgorithm\\forestMachineLearning\\data\\processes_train.csv\"\n",
        "VALID_CSV = r\"D:\\Project\\MachineLearningAlgorithm\\forestMachineLearning\\data\\processes_valid.csv\"  # 若不存在，将从训练集切分\n",
        "TEST_CSV  = r\"D:\\Project\\MachineLearningAlgorithm\\forestMachineLearning\\data\\processes_test.csv\"\n",
        "\n",
        "# 提交文件设置\n",
        "SUBMIT_ID_COL_CANDIDATES = [\"Id\", \"id\", \"index\"]  # 自动优先使用存在的列作为ID\n",
        "SUBMIT_SCORE_COL = \"anomaly_score\"               # 提交分数列名\n",
        "SUBMIT_PATH = r\"D:\\Project\\MachineLearningAlgorithm\\forestMachineLearning\\data\\submission_isoforest.csv\"\n",
        "\n",
        "# 预处理开关\n",
        "PROCESS_ARGS = False   # 是否展开 args（默认 False，仅保留 argsNum）\n",
        "DROP_RAW_COLS = True   # 是否删除原始大字段，如 args / stackAddresses\n",
        "\n",
        "# 归一化设置\n",
        "NORM_METHOD = \"minmax\"   # \"minmax\" | \"rank\"\n",
        "\n",
        "# 训练/验证设置\n",
        "RANDOM_STATE = 42\n",
        "VALID_RATIO = 0.2        # 若没有独立验证集，将从训练集中切分\n",
        "USE_LABELS_IF_AVAILABLE = True  # 如果存在 target 列，则用于评估（不参与无监督训练）\n",
        "\n",
        "# 模型超参\n",
        "from sklearn.ensemble import IsolationForest\n",
        "ISO_PARAMS = dict(\n",
        "    n_estimators=150,\n",
        "    contamination=0.04,   # 如有标签，可用训练集阳性占比近似设置\n",
        "    max_features=1.0,\n",
        "    max_samples=20000,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# 评估与阈值\n",
        "THRESHOLD_STRATEGY = \"percentile\"  # \"percentile\" | \"contamination\"\n",
        "THRESHOLD_PERCENTILE = 97.0        # 当使用 percentile 时生效\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 工具函数与特征工程"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Tuple, List, Dict, Optional\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# 固定特征列（与项目对齐）\n",
        "REQ_FEATURES = [\n",
        "    \"timestamp\",         # 连续值，后续会做相对化\n",
        "    \"processId\",         # 连续/整数\n",
        "    \"parentProcessId\",   # 连续/整数\n",
        "    \"userId\",            # 连续/整数\n",
        "    \"mountNamespace\",    # 连续/整数/类别\n",
        "    \"processName\",       # 类别\n",
        "    \"hostName\",          # 类别\n",
        "    \"eventName\",         # 类别\n",
        "    \"argsNum\",           # 连续/计数\n",
        "    \"returnValue\",       # 连续/整数\n",
        "    \"stack_depth\"        # 由 stackAddresses 衍生\n",
        "]\n",
        "\n",
        "CATEGORICAL_COLS = [\"processName\", \"hostName\", \"eventName\"]\n",
        "\n",
        "def compute_stack_depth(df: pd.DataFrame) -> pd.Series:\n",
        "    if \"stack_depth\" in df.columns:\n",
        "        return df[\"stack_depth\"]\n",
        "    if \"stackAddresses\" in df.columns:\n",
        "        # stackAddresses 可能是字符串 \"[]\" 或 list\n",
        "        def _len_safe(x):\n",
        "            if isinstance(x, list):\n",
        "                return len(x)\n",
        "            if isinstance(x, str):\n",
        "                if x.strip() == \"\" or x.strip() == \"[]\":\n",
        "                    return 0\n",
        "                # 粗略判断逗号个数\n",
        "                return x.count(\",\") + 1 if \"[\" in x and \"]\" in x else 0\n",
        "            return 0\n",
        "        return df[\"stackAddresses\"].apply(_len_safe).astype(\"int64\")\n",
        "    # 都没有时，补 0\n",
        "    return pd.Series(0, index=df.index, dtype=\"int64\")\n",
        "\n",
        "def make_timestamp_relative(df: pd.DataFrame) -> pd.Series:\n",
        "    # 按 processId 分组：timestamp - min(timestamp)\n",
        "    if \"timestamp\" not in df.columns or \"processId\" not in df.columns:\n",
        "        return df.get(\"timestamp\", pd.Series(0.0, index=df.index))\n",
        "    ts = df[\"timestamp\"].astype(\"float64\")\n",
        "    pid = df[\"processId\"]\n",
        "    rel = ts - ts.groupby(pid).transform(\"min\")\n",
        "    return rel\n",
        "\n",
        "def basic_clean(df: pd.DataFrame, process_args: bool = False, drop_raw_cols: bool = True) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    # 衍生 stack_depth\n",
        "    df[\"stack_depth\"] = compute_stack_depth(df)\n",
        "    # timestamp 相对化\n",
        "    if \"timestamp\" in df.columns:\n",
        "        df[\"timestamp\"] = make_timestamp_relative(df)\n",
        "    # argsNum 尽量保证存在\n",
        "    if \"argsNum\" not in df.columns and \"args\" in df.columns:\n",
        "        def _len_args(a):\n",
        "            if isinstance(a, list):\n",
        "                return len(a)\n",
        "            if isinstance(a, str):\n",
        "                return a.count(\"name\") if \"name\" in a else 0\n",
        "            return 0\n",
        "        try:\n",
        "            df[\"argsNum\"] = df[\"args\"].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
        "        except Exception:\n",
        "            df[\"argsNum\"] = 0\n",
        "\n",
        "    # 可选：删除大字段\n",
        "    if drop_raw_cols:\n",
        "        for col in [\"args\", \"stackAddresses\"]:\n",
        "            if col in df.columns:\n",
        "                del df[col]\n",
        "    return df\n",
        "\n",
        "def ensure_req_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # 缺失列补 NaN / 0；类型尽量转为数值/字符串\n",
        "    out = df.copy()\n",
        "    if \"stack_depth\" not in out.columns:\n",
        "        out[\"stack_depth\"] = 0\n",
        "    for c in REQ_FEATURES:\n",
        "        if c not in out.columns:\n",
        "            out[c] = np.nan\n",
        "    # 类型标准化\n",
        "    for c in [\"processId\", \"parentProcessId\", \"userId\", \"mountNamespace\", \"argsNum\", \"returnValue\", \"stack_depth\"]:\n",
        "        if c in out.columns:\n",
        "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
        "    for c in [\"processName\", \"hostName\", \"eventName\"]:\n",
        "        if c in out.columns:\n",
        "            out[c] = out[c].astype(str)\n",
        "    return out[REQ_FEATURES]\n",
        "\n",
        "def fit_label_encoders(df_list: List[pd.DataFrame], cat_cols: List[str]) -> Dict[str, LabelEncoder]:\n",
        "    encoders = {}\n",
        "    for c in cat_cols:\n",
        "        le = LabelEncoder()\n",
        "        # 合并拟合，避免验证/测试集出现未知值\n",
        "        vals = pd.concat([d[c].astype(str) for d in df_list if c in d.columns], axis=0)\n",
        "        le.fit(vals.fillna(\"\"))\n",
        "        encoders[c] = le\n",
        "    return encoders\n",
        "\n",
        "def apply_label_encoders(df: pd.DataFrame, encoders: Dict[str, LabelEncoder]) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for c, le in encoders.items():\n",
        "        if c in df.columns:\n",
        "            df[c] = le.transform(df[c].astype(str).fillna(\"\"))\n",
        "    return df\n",
        "\n",
        "def z01(scores: np.ndarray, method: str = \"minmax\") -> np.ndarray:\n",
        "    s = np.asarray(scores, dtype=float)\n",
        "    if method == \"minmax\":\n",
        "        lo, hi = np.nanmin(s), np.nanmax(s)\n",
        "        if hi - lo < 1e-12:\n",
        "            return np.zeros_like(s)\n",
        "        return (s - lo) / (hi - lo)\n",
        "    elif method == \"rank\":\n",
        "        # 分位数映射到 [0,1]\n",
        "        ranks = pd.Series(s).rank(method=\"average\") / len(s)\n",
        "        return ranks.to_numpy()\n",
        "    else:\n",
        "        raise ValueError(\"Unknown norm method\")\n",
        "\n",
        "def choose_id_column(df: pd.DataFrame, candidates: List[str]) -> str:\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    # 兜底：保存一个 index 列\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 读取数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "使用独立验证集： D:\\Project\\MachineLearningAlgorithm\\forestMachineLearning\\data\\processes_valid.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>target</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>processId</th>\n",
              "      <th>threadId</th>\n",
              "      <th>parentProcessId</th>\n",
              "      <th>userId</th>\n",
              "      <th>mountNamespace</th>\n",
              "      <th>processName</th>\n",
              "      <th>hostName</th>\n",
              "      <th>eventId</th>\n",
              "      <th>eventName</th>\n",
              "      <th>stackAddresses</th>\n",
              "      <th>argsNum</th>\n",
              "      <th>returnValue</th>\n",
              "      <th>args</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>124.439221</td>\n",
              "      <td>381</td>\n",
              "      <td>381</td>\n",
              "      <td>1</td>\n",
              "      <td>101</td>\n",
              "      <td>4026532232</td>\n",
              "      <td>systemd-resolve</td>\n",
              "      <td>0</td>\n",
              "      <td>41</td>\n",
              "      <td>socket</td>\n",
              "      <td>[139913106282763, 139913103116537, 94901962555...</td>\n",
              "      <td>3</td>\n",
              "      <td>15</td>\n",
              "      <td>[{'name': 'domain', 'type': 'int', 'value': 'A...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>124.439958</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4026531840</td>\n",
              "      <td>systemd</td>\n",
              "      <td>0</td>\n",
              "      <td>1005</td>\n",
              "      <td>security_file_open</td>\n",
              "      <td>[140074839310116, 8103505641674583864]</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>[{'name': 'pathname', 'type': 'const char*', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>124.440037</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4026531840</td>\n",
              "      <td>systemd</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>fstat</td>\n",
              "      <td>[140074839307913]</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>[{'name': 'fd', 'type': 'int', 'value': 12}, {...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  target   timestamp  processId  threadId  parentProcessId  userId  \\\n",
              "0      0       0  124.439221        381       381                1     101   \n",
              "1      2       0  124.439958          1         1                0       0   \n",
              "2      4       0  124.440037          1         1                0       0   \n",
              "\n",
              "   mountNamespace      processName  hostName  eventId           eventName  \\\n",
              "0      4026532232  systemd-resolve         0       41              socket   \n",
              "1      4026531840          systemd         0     1005  security_file_open   \n",
              "2      4026531840          systemd         0        5               fstat   \n",
              "\n",
              "                                      stackAddresses  argsNum  returnValue  \\\n",
              "0  [139913106282763, 139913103116537, 94901962555...        3           15   \n",
              "1             [140074839310116, 8103505641674583864]        4            0   \n",
              "2                                  [140074839307913]        2            0   \n",
              "\n",
              "                                                args  \n",
              "0  [{'name': 'domain', 'type': 'int', 'value': 'A...  \n",
              "1  [{'name': 'pathname', 'type': 'const char*', '...  \n",
              "2  [{'name': 'fd', 'type': 'int', 'value': 12}, {...  "
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_data(train_csv, valid_csv, test_csv, valid_ratio=0.2, random_state=42):\n",
        "    train_df = pd.read_csv(train_csv) if os.path.exists(train_csv) else None\n",
        "    valid_df = pd.read_csv(valid_csv) if os.path.exists(valid_csv) else None\n",
        "    test_df  = pd.read_csv(test_csv)  if os.path.exists(test_csv)  else None\n",
        "\n",
        "    if train_df is None:\n",
        "        raise FileNotFoundError(f\"训练集不存在：{train_csv}\")\n",
        "\n",
        "    if valid_df is None:\n",
        "        # 从训练集切分\n",
        "        if 'target' in train_df.columns:\n",
        "            train_part, valid_part = train_test_split(\n",
        "                train_df, test_size=valid_ratio, stratify=train_df['target'], random_state=random_state\n",
        "            )\n",
        "        else:\n",
        "            train_part, valid_part = train_test_split(\n",
        "                train_df, test_size=valid_ratio, random_state=random_state\n",
        "            )\n",
        "        train_df, valid_df = train_part.reset_index(drop=True), valid_part.reset_index(drop=True)\n",
        "        print(f\"未发现独立验证集，已从训练集按 {valid_ratio:.0%} 切分。\")\n",
        "    else:\n",
        "        print(\"使用独立验证集：\", valid_csv)\n",
        "\n",
        "    if test_df is None:\n",
        "        print(\"警告：未发现测试集，将仅进行训练与验证。\")\n",
        "    return train_df.reset_index(drop=True), valid_df.reset_index(drop=True), (None if test_df is None else test_df.reset_index(drop=True))\n",
        "\n",
        "train_df_raw, valid_df_raw, test_df_raw = load_data(TRAIN_CSV, VALID_CSV, TEST_CSV, VALID_RATIO, RANDOM_STATE)\n",
        "train_df_raw.head(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 预处理（对齐特征）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape: (638720, 11)\n",
            "Valid shape: (102160, 11)\n",
            "Test shape: (259293, 11)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>processId</th>\n",
              "      <th>parentProcessId</th>\n",
              "      <th>userId</th>\n",
              "      <th>mountNamespace</th>\n",
              "      <th>processName</th>\n",
              "      <th>hostName</th>\n",
              "      <th>eventName</th>\n",
              "      <th>argsNum</th>\n",
              "      <th>returnValue</th>\n",
              "      <th>stack_depth</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>381</td>\n",
              "      <td>1</td>\n",
              "      <td>101</td>\n",
              "      <td>4026532232</td>\n",
              "      <td>91</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>3</td>\n",
              "      <td>15</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4026531840</td>\n",
              "      <td>86</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000079</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4026531840</td>\n",
              "      <td>86</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   timestamp  processId  parentProcessId  userId  mountNamespace  processName  \\\n",
              "0   0.000000        381                1     101      4026532232           91   \n",
              "1   0.000000          1                0       0      4026531840           86   \n",
              "2   0.000079          1                0       0      4026531840           86   \n",
              "\n",
              "   hostName  eventName  argsNum  returnValue  stack_depth  \n",
              "0         0         40        3           15            3  \n",
              "1         0         32        4            0            2  \n",
              "2         0         17        2            0            1  "
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# 基础清洗\n",
        "train_df = basic_clean(train_df_raw, process_args=PROCESS_ARGS, drop_raw_cols=DROP_RAW_COLS)\n",
        "valid_df = basic_clean(valid_df_raw, process_args=PROCESS_ARGS, drop_raw_cols=DROP_RAW_COLS)\n",
        "test_df  = None if test_df_raw is None else basic_clean(test_df_raw, process_args=PROCESS_ARGS, drop_raw_cols=DROP_RAW_COLS)\n",
        "\n",
        "# 对齐必需特征\n",
        "train_df = ensure_req_features(train_df)\n",
        "valid_df = ensure_req_features(valid_df)\n",
        "if test_df is not None:\n",
        "    test_df = ensure_req_features(test_df)\n",
        "\n",
        "# 类别编码（使用 训练+验证+测试 的并集拟合）\n",
        "enc_fit_list = [train_df, valid_df] + ([test_df] if test_df is not None else [])\n",
        "encoders = fit_label_encoders(enc_fit_list, CATEGORICAL_COLS)\n",
        "train_enc = apply_label_encoders(train_df, encoders)\n",
        "valid_enc = apply_label_encoders(valid_df, encoders)\n",
        "test_enc  = None if test_df is None else apply_label_encoders(test_df, encoders)\n",
        "\n",
        "print('Train shape:', train_enc.shape)\n",
        "print('Valid shape:', valid_enc.shape)\n",
        "if test_enc is not None:\n",
        "    print('Test shape:', test_enc.shape)\n",
        "\n",
        "train_enc.head(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 训练 IsolationForest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "iso = IsolationForest(**ISO_PARAMS)\n",
        "iso.fit(train_enc.values)  # 无监督训练，不用 y\n",
        "\n",
        "# 保存模型（可选）\n",
        "import pickle, os\n",
        "# MODEL_PATH = os.path.join(DATA_DIR, \"isoforest_model.pkl\")\n",
        "# with open(MODEL_PATH, \"wb\") as f:\n",
        "#     pickle.dump(dict(model=iso, encoders=encoders, req_features=REQ_FEATURES), f)\n",
        "# print(\"模型已保存：\", MODEL_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 验证集打分与评估"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation metrics: {'AP': 0.9065966655906399, 'Precision': 0.928897586431833, 'Recall': 0.7311938382541721, 'F1': 0.8182732366039362, 'thr': np.float64(0.6859453803171673)}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.644011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.352499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.550478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.567437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.618471</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      score\n",
              "0  0.644011\n",
              "1  0.352499\n",
              "2  0.550478\n",
              "3  0.567437\n",
              "4  0.618471"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 原始分数：scikit-learn 中 score_samples 越大越正常 => 我们将其取反作为异常分数\n",
        "raw_scores_valid = -iso.score_samples(valid_enc.values)\n",
        "# 归一化到 [0,1]\n",
        "z_valid = z01(raw_scores_valid, method=NORM_METHOD)\n",
        "\n",
        "metrics = {}\n",
        "if USE_LABELS_IF_AVAILABLE and ('target' in valid_df_raw.columns):\n",
        "    from sklearn.metrics import average_precision_score, precision_recall_fscore_support\n",
        "\n",
        "    y_true = valid_df_raw['target'].to_numpy().astype(int)\n",
        "    ap = average_precision_score(y_true, z_valid)\n",
        "\n",
        "    # 阈值策略\n",
        "    if THRESHOLD_STRATEGY == \"percentile\":\n",
        "        thr = np.percentile(z_valid, THRESHOLD_PERCENTILE)\n",
        "    elif THRESHOLD_STRATEGY == \"contamination\":\n",
        "        # 使用训练参数 contamination 作为阈值占比\n",
        "        frac = ISO_PARAMS.get(\"contamination\", 0.05)\n",
        "        thr = np.percentile(z_valid, 100 * (1 - frac))\n",
        "    else:\n",
        "        thr = np.percentile(z_valid, 97.0)\n",
        "\n",
        "    y_pred = (z_valid >= thr).astype(int)\n",
        "    P, R, F1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
        "    metrics = dict(AP=ap, Precision=P, Recall=R, F1=F1, thr=thr)\n",
        "    print(\"Validation metrics:\", metrics)\n",
        "else:\n",
        "    print(\"验证集无标签（target），仅输出分数统计。\")\n",
        "    print(pd.Series(z_valid).describe())\n",
        "\n",
        "# 输出前几行查看\n",
        "pd.DataFrame({\"score\": z_valid}).head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 测试集打分与生成 Submission CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "e00a5786",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import (\n",
        "    average_precision_score,\n",
        "    precision_recall_fscore_support,\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        ")\n",
        "\n",
        "def validate_isoforest(\n",
        "    valid_df_raw: pd.DataFrame,\n",
        "    valid_enc: pd.DataFrame,\n",
        "    model,\n",
        "    norm_method: str = \"minmax\",          # \"minmax\" 或 \"rank\"\n",
        "    threshold_strategy: str = \"percentile\",# \"percentile\" 或 \"contamination\"\n",
        "    threshold_percentile: float = 97.0,\n",
        "    contamination: float = 0.04,\n",
        "    positive_label: int = 1,\n",
        "    print_report: bool = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    使用验证集评估无监督异常检测（IsolationForest）。\n",
        "    要求 valid_df_raw 中包含 'target' 列（0/1）。\n",
        "    参数:\n",
        "      - valid_df_raw: 原始验证集（含 target）\n",
        "      - valid_enc:    已完成特征对齐与编码的验证集特征矩阵（与训练同构）\n",
        "      - model:        已 fit 的 IsolationForest\n",
        "      - norm_method:  分数归一化方法（\"minmax\" 或 \"rank\"）\n",
        "      - threshold_strategy:\n",
        "            * \"percentile\": 用验证集分数的百分位设阈值（默认 97%）\n",
        "            * \"contamination\": 用污染率设置阈值（百分位 = 100 * (1 - contamination)）\n",
        "      - contamination: 当 threshold_strategy=\"contamination\" 时使用\n",
        "      - positive_label: 正类标签（异常=1）\n",
        "    返回:\n",
        "      - metrics: dict，包含 AP, Precision, Recall, F1, Accuracy, Threshold, 混淆矩阵等\n",
        "      - z: np.ndarray，归一化后的异常分数（越大越异常）\n",
        "    \"\"\"\n",
        "    if \"target\" not in valid_df_raw.columns:\n",
        "        raise ValueError(\"valid_df_raw 必须包含 'target' 列用于评估。\")\n",
        "\n",
        "    # 1) 原始异常分数：sklearn 的 score_samples 越大越正常，所以取反\n",
        "    raw = -model.score_samples(valid_enc.values)\n",
        "\n",
        "    # 2) 归一化到 [0,1]\n",
        "    if norm_method == \"minmax\":\n",
        "        lo, hi = np.nanmin(raw), np.nanmax(raw)\n",
        "        z = np.zeros_like(raw) if hi - lo < 1e-12 else (raw - lo) / (hi - lo)\n",
        "    elif norm_method == \"rank\":\n",
        "        z = pd.Series(raw).rank(method=\"average\").to_numpy() / len(raw)\n",
        "    else:\n",
        "        raise ValueError(\"norm_method 仅支持 'minmax' 或 'rank'。\")\n",
        "\n",
        "    # 3) AP（不依赖阈值）\n",
        "    y_true = valid_df_raw[\"target\"].to_numpy().astype(int)\n",
        "    ap = average_precision_score(y_true, z)\n",
        "\n",
        "    # 4) 选阈值并二分类\n",
        "    if threshold_strategy == \"percentile\":\n",
        "        thr = np.percentile(z, threshold_percentile)\n",
        "    elif threshold_strategy == \"contamination\":\n",
        "        thr = np.percentile(z, 100 * (1 - contamination))\n",
        "    else:\n",
        "        raise ValueError(\"threshold_strategy 仅支持 'percentile' 或 'contamination'。\")\n",
        "\n",
        "    y_pred = (z >= thr).astype(int)\n",
        "\n",
        "    # 5) 指标与混淆矩阵\n",
        "    P, R, F1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=\"binary\", pos_label=positive_label, zero_division=0\n",
        "    )\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
        "\n",
        "    metrics = {\n",
        "        \"AP\": float(ap),\n",
        "        \"Precision\": float(P),\n",
        "        \"Recall\": float(R),\n",
        "        \"F1\": float(F1),\n",
        "        \"Accuracy\": float(acc),\n",
        "        \"Threshold\": float(thr),\n",
        "        \"TP\": int(tp),\n",
        "        \"FP\": int(fp),\n",
        "        \"TN\": int(tn),\n",
        "        \"FN\": int(fn),\n",
        "        \"Strategy\": threshold_strategy,\n",
        "        \"Percentile(%)\": threshold_percentile if threshold_strategy == \"percentile\" else None,\n",
        "        \"Contamination\": contamination if threshold_strategy == \"contamination\" else None,\n",
        "        \"Norm\": norm_method,\n",
        "    }\n",
        "\n",
        "    if print_report:\n",
        "        print(\n",
        "            f\"AP={metrics['AP']:.4f} | P={metrics['Precision']:.4f} \"\n",
        "            f\"R={metrics['Recall']:.4f} F1={metrics['F1']:.4f} Acc={metrics['Accuracy']:.4f} \"\n",
        "            f\"| thr={metrics['Threshold']:.6f} ({metrics['Strategy']})\"\n",
        "        )\n",
        "        print(f\"Confusion Matrix: TP={tp}, FP={fp}, TN={tn}, FN={fn}\")\n",
        "\n",
        "    return metrics, z\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "已保存提交文件： D:\\Project\\MachineLearningAlgorithm\\forestMachineLearning\\data\\submission_isoforest.csv\n",
            "AP=0.9066 | P=0.9289 R=0.7312 F1=0.8183 Acc=0.9876 | thr=0.685945 (percentile)\n",
            "Confusion Matrix: TP=2848, FP=218, TN=98047, FN=1047\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'AP': 0.9065966655906399,\n",
              " 'Precision': 0.928897586431833,\n",
              " 'Recall': 0.7311938382541721,\n",
              " 'F1': 0.8182732366039362,\n",
              " 'Accuracy': 0.9876174628034455,\n",
              " 'Threshold': 0.6859453803171673,\n",
              " 'TP': 2848,\n",
              " 'FP': 218,\n",
              " 'TN': 98047,\n",
              " 'FN': 1047,\n",
              " 'Strategy': 'percentile',\n",
              " 'Percentile(%)': 97.0,\n",
              " 'Contamination': None,\n",
              " 'Norm': 'minmax'}"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "if test_enc is None:\n",
        "    print(\"未提供测试集，跳过提交文件生成。\")\n",
        "else:\n",
        "    raw_scores_test = -iso.score_samples(test_enc.values)\n",
        "    z_test = z01(raw_scores_test, method=NORM_METHOD)\n",
        "\n",
        "    # 选择 ID 列\n",
        "    id_col = choose_id_column(test_df_raw, SUBMIT_ID_COL_CANDIDATES)\n",
        "    if id_col is None:\n",
        "        sub = pd.DataFrame({\n",
        "            \"index\": np.arange(len(test_df_raw)),\n",
        "            SUBMIT_SCORE_COL: z_test\n",
        "        })\n",
        "    else:\n",
        "        sub = pd.DataFrame({\n",
        "            id_col: test_df_raw[id_col].values,\n",
        "            SUBMIT_SCORE_COL: z_test\n",
        "        })\n",
        "    # sub.to_csv(SUBMIT_PATH, index=False)\n",
        "    print(\"已保存提交文件：\", SUBMIT_PATH)\n",
        "    sub.head(10)\n",
        "\n",
        "\n",
        "# 假设你已经有：\n",
        "# - valid_df_raw  原始验证集（含 target）\n",
        "# - valid_enc     编码后的验证特征\n",
        "# - iso           训练好的 IsolationForest 模型\n",
        "\n",
        "metrics, z_valid = validate_isoforest(\n",
        "    valid_df_raw=valid_df_raw,\n",
        "    valid_enc=valid_enc,\n",
        "    model=iso,\n",
        "    norm_method=\"minmax\",\n",
        "    threshold_strategy=\"percentile\",  # 或 \"contamination\"\n",
        "    threshold_percentile=97.0,\n",
        "    contamination=0.04,\n",
        "    print_report=True\n",
        ")\n",
        "\n",
        "metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. （可选）封装推理函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "一致性检查： True\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def infer_scores(df_raw: pd.DataFrame, model, encoders, req_features=REQ_FEATURES, norm_method=\"minmax\"):\n",
        "    df = basic_clean(df_raw, process_args=PROCESS_ARGS, drop_raw_cols=DROP_RAW_COLS)\n",
        "    df = ensure_req_features(df)\n",
        "    df = apply_label_encoders(df, encoders)\n",
        "    raw_scores = -model.score_samples(df.values)\n",
        "    return z01(raw_scores, method=norm_method)\n",
        "\n",
        "# 示例：对验证集复算\n",
        "_z = infer_scores(valid_df_raw, iso, encoders, req_features=REQ_FEATURES, norm_method=NORM_METHOD)\n",
        "print(\"一致性检查：\", np.allclose(_z, z01(-iso.score_samples(valid_enc.values), method=NORM_METHOD)))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "p310cuda12.7",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
